#miscellaneous_qqplot <- qqnorm(manydogs_missing_handled$miscellaneous_score)
#qqline(manydogs_missing_handled$miscellaneous_score)
#miscellaneous_density <- plot(density(manydogs_missing_handled$miscellaneous_score), main = "Density Plot of Miscellaneous")
#Plots for Separation
#separation_qqplot <- qqnorm(manydogs_missing_handled$separation_score)
#qqline(manydogs_missing_handled$separation_score)
#separation_density <- plot(density(manydogs_missing_handled$separation_score), main = "Density Plot of Separation")
#Create a boxplot for age
age_boxplot <- boxplot(manydogs_missing_handled$age)
#Pull out the values of the outliers
age_boxplot$out
#Basic logistic regression model with training_score predictor variable and outcome variable of whether the dog did above chance on the ostensive task
model_RQ_1 <- glm(ostensive_binary ~ training_score, data = manydogs_missing_handled, family = binomial)
crPlots(model_RQ_1, terms = ~training_score,
pch=20, col="gray",
smooth = list(smoother=car::gamLine))
#Make basic logistic regression model with training_score predictor variable and outcome variable of whether the dog did above chance on the nonostensive task
model_RQ_2 <- glm(nonostensive_binary ~ training_score, data = manydogs_missing_handled, family = binomial)
crPlots(model_RQ_2, terms = ~training_score,
pch=20, col="gray",
smooth = list(smoother=car::gamLine))
#Make basic logistic regression model with all predictors and outcome variable that measures whether the dog did better at the nonostensive task
model_RQ_3 <- glm(nonos_best ~ age + sex + desexed + purebred + gaze_follow + training_score + aggression_score + fear_score + separation_score + excitability_score + attachment_score + miscellaneous_score, data = manydogs_missing_handled, family = binomial)
age_crplot <- crPlots(model_RQ_3, terms = ~age,
pch=20, col="gray",
smooth = list(smoother=car::gamLine))
#Make basic logistic regression model with all predictors and outcome variable that measures whether the dog did better at the nonostensive task
model_RQ_3 <- glm(nonos_best ~ age + sex + desexed + purebred + gaze_follow + training_score + aggression_score + fear_score + separation_score + excitability_score + attachment_score + miscellaneous_score, data = manydogs_missing_handled, family = binomial)
age_crplot <- crPlots(model_RQ_3, terms = ~age,
pch=20, col="gray",
smooth = list(smoother=car::gamLine))
#training_crplot <- crPlots(model_RQ_3, terms = ~training_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
#aggression_crplot <- crPlots(model_RQ_3, terms = ~aggression_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
#fear_crplot <- crPlots(model_RQ_3, terms = ~fear_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
#separation_crplot <- crPlots(model_RQ_3, terms = ~separation_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
#excitability_crplot <- crPlots(model_RQ_3, terms = ~excitability_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
#attachment_crplot <- crPlots(model_RQ_3, terms = ~attachment_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
#miscellaneous_crplot <- crPlots(model_RQ_3, terms = ~miscellaneous_score,
#pch=20, col="gray",
#smooth = list(smoother=car::gamLine))
manydogs_transformed <- manydogs_missing_handled %>%
mutate_at(vars(age, training_score, aggression_score, fear_score, separation_score, excitability_score, attachment_score, miscellaneous_score), scale)
write.csv(manydogs_transformed, file = "manydogs_transformed.csv")
library(knitr)
manydogs_transformed <- read.csv("manydogs_transformed.csv")
set.seed(1234567) # ensures randomized outputs are reproducible
train_indices <- sample(1:nrow(manydogs_transformed), 0.8 * nrow(manydogs_transformed)) #Grabs a random subset of the data. In this case we asked for 80% of the data to be sampled which is the standard
training_data <- manydogs_transformed[train_indices, ] #Grab the 80% of the data you just pulled out and call it training
testing_data <- manydogs_transformed[-train_indices, ] # Grab the other 20% not in the train indices and call it testing
write.csv(training_data, file = "training_data.csv")
write.csv(testing_data, file = "testing_data.csv")
View(training_data)
library(knitr)
library(mlr)
training_data <- read.csv("training_data.csv")
knitr::include_graphics("nesting_dolls.png")
loocv <- makeResampleDesc(method = "LOO") #define parameters for cross validation
#10fold_cross_validation <- makeResampleDesc(method = "RepCV", folds = 10, reps = 10, stratify = TRUE) #If you want a different number of folds you can change the number to anything you like. If your number of folds is the same number as your observations than you have remade LOOCV!
library(knitr)
library(knitr)
library(mlr)
library(tidyverse)
set.seed(1234567)
testing_data <- read.csv("testing_data.csv")
KNN_model_RQ_1 <- readRDS("KNN_model_RQ_1.rds")
decision_tree_model_RQ_1 <- readRDS("decision_tree_model_RQ_1.rds")
SVM_model_RQ_1 <- readRDS("SVM_model_RQ_1.rds")
KNN_model_RQ_2 <- readRDS("KNN_model_RQ_2.rds")
decision_tree_model_RQ_2 <- readRDS("decision_tree_model_RQ_2.rds")
SVM_model_RQ_2 <- readRDS("SVM_model_RQ_2.rds")
KNN_model_RQ_3 <- readRDS("KNN_model_RQ_3.rds")
random_forest_model_RQ_3 <- readRDS("random_forest_model_RQ_3.rds")
library(mlr)
library(tidyverse)
library(parallelMap)
library(parallel)
library(e1071)
set.seed(1234567)
training_data <- read.csv("training_data.csv")
#Research Question #1
#extract only the two columns from the large pre-processed training dataset we made in step 1 that are needed for the research question: training score and the ostensive binary outcome variable
data_RQ_1 <- training_data |>
select(training_score, ostensive_binary) |>
mutate(ostensive_binary = as.factor(ostensive_binary))
#Define the Task for RQ #1
task_RQ_1 <- makeClassifTask(data = data_RQ_1, target = "ostensive_binary")
#Research Question #2
#extract only the two columns from the large pre-processed dataset that are needed for the research question: training score and the nonostensive binary outcome variable
data_RQ_2 <- training_data |>
select(training_score, nonostensive_binary) |>
mutate(nonostensive_binary = as.factor(nonostensive_binary))
#Define the task for RQ #2
task_RQ_2 <- makeClassifTask(data = data_RQ_2, target = "nonostensive_binary")
#Research Question #3
#select the predictor variables we will be using and the outcome variable of os_best
data_RQ_3_factor <- training_data |>
select(sex:miscellaneous_score,nonos_best) |>
mutate(across(c(sex, desexed, purebred, gaze_follow, nonos_best), as.factor))
data_RQ_3_numeric <- data_RQ_3_factor |>
mutate(across(c(sex, desexed, purebred, gaze_follow), as.numeric))
#Define the task for RQ #3
task_RQ_3_factor <- makeClassifTask(data = data_RQ_3_factor, target = "nonos_best")
task_RQ_3_numeric <- makeClassifTask(data = data_RQ_3_numeric, target = "nonos_best")
#Creating the KNN learner for RQ1
knn_learner_RQ_1 <- makeLearner("classif.knn")
#finding the best value of k
#define values to check. Best to check 1 through 10 to start.
knnParamSpace_RQ_1 <- makeParamSet(makeDiscreteParam("k", values = 1:10))
#tell the computer to search every value of k we offered (1 thru 10)
gridSearch_RQ_1 <- makeTuneControlGrid()
#cross validate for the k finding
cvForTuning <- makeResampleDesc("RepCV", folds = 10, reps =20)
#Use the tuneParams function and the task we made above to get the best k value
tunedk_RQ_1 <- suppressMessages({
tuneParams("classif.knn", task = task_RQ_1, resampling = cvForTuning,
par.set = knnParamSpace_RQ_1, control = gridSearch_RQ_1, show.info = F)
})
tunedk_RQ_1
#Creating the KNN learner for RQ1
knn_learner_RQ_1 <- makeLearner("classif.knn", par.vals = list("k"=9))
#Training the model while cross validating
KNN_model_RQ_1 <- suppressMessages({ train(learner = knn_learner_RQ_1, task = task_RQ_1)
})
loocv <- makeResampleDesc(method = "LOO") #cross validation procedure
#Get a sense of how well the model will work in practice with cross validation
KNN_model_RQ_1_loocv <- suppressMessages({
resample(learner = knn_learner_RQ_1, task = task_RQ_1,
resampling = loocv, measures = list(mmce, acc))
})
saveRDS(KNN_model_RQ_1, file = "KNN_model_RQ_1.rds")
#Performance
KNN_model_RQ_1_loocv$aggr
#Creating the learner for a Single decision tree
decisiontree_learner_RQ_1_and_2 <- makeLearner("classif.rpart")
#Training the model
decision_tree_model_RQ_1 <- suppressMessages({
train(learner = decisiontree_learner_RQ_1_and_2,
task = task_RQ_1)
})
#Checking how well the model does using cross validating
decision_tree_model_RQ_1_loocv <- suppressMessages({
resample(learner = decisiontree_learner_RQ_1_and_2,
task = task_RQ_1, resampling = loocv,
measures = list(mmce, acc))
})
saveRDS(decision_tree_model_RQ_1, file = "decision_tree_model_RQ_1.rds")
#Performance of the Decision Tree
decision_tree_model_RQ_1_loocv$aggr
View(task_RQ_1)
View(data_RQ_1)
View(data_RQ_2)
View(data_RQ_3_numeric)
#Create vector listing the possible kernels you can
kernels <- c("polynomial", "radial", "sigmoid")
svm_parametercheck <- makeParamSet(makeDiscreteParam("kernel", values = kernels),
makeIntegerParam("degree", lower =1, upper = 3),
makeNumericParam("cost", lower = 0.1, upper =10),
makeNumericParam("gamma", lower = 0.1, upper = 10))
randomsearch <- makeTuneControlRandom(maxit = 50)
#use all of the cpus on your computer
parallelStartSocket(cpus = detectCores())
#cross validation procedure for tuning this SVM algorithm
cvForTuning2 <- makeResampleDesc("RepCV", folds = 5, reps =5)
#Tune the hyperparameters
tunedSvmPars_RQ1 <- suppressMessages({
tuneParams("classif.svm", task = task_RQ_1,
resampling = cvForTuning2,
par.set = svm_parametercheck,
control = randomsearch)
})
#use all of the cpus on your computer
parallelStartSocket(cpus = detectCores())
#cross validation procedure for tuning this SVM algorithm
cvForTuning2 <- makeResampleDesc("RepCV", folds = 5, reps =5)
#Tune the hyperparameters
tunedSvmPars_RQ1 <- suppressMessages({
tuneParams("classif.svm", task = task_RQ_1,
resampling = cvForTuning2,
par.set = svm_parametercheck,
control = randomsearch)
})
tunedSvmPars_RQ1
#use all of the cpus on your computer
parallelStartSocket(cpus = detectCores())
#cross validation procedure for tuning this SVM algorithm
cvForTuning2 <- makeResampleDesc("RepCV", folds = 5, reps =5)
#Tune the hyperparameters
tunedSvmPars_RQ1 <- suppressMessages({
tuneParams("classif.svm", task = task_RQ_1,
resampling = cvForTuning2,
par.set = svm_parametercheck,
control = randomsearch)
})
tunedSvmPars_RQ1
#use all of the cpus on your computer
parallelStartSocket(cpus = detectCores())
#cross validation procedure for tuning this SVM algorithm
cvForTuning2 <- makeResampleDesc("RepCV", folds = 5, reps =5)
#Tune the hyperparameters
tunedSvmPars_RQ1 <- suppressMessages({
tuneParams("classif.svm", task = task_RQ_1,
resampling = cvForTuning2,
par.set = svm_parametercheck,
control = randomsearch)
})
tunedSvmPars_RQ1
#Make the SVM Learner
SVM_learner_tuned_RQ_1 <- makeLearner("classif.svm", par.vals = tunedSvmPars_RQ1$x)
#Train the model
SVM_model_RQ_1 <- suppressMessages({
train(learner = SVM_learner_tuned_RQ_1, task = task_RQ_1)
})
#Check model performance with LOOCV
SVM_model_RQ_1_loocv <- suppressMessages({
resample(learner = SVM_learner_tuned_RQ_1,
task = task_RQ_1,
resampling = loocv,
measures = list(mmce, acc))
})
#Output the Lost/Cost functions for this model
SVM_model_RQ_1_loocv$aggr
#finding the best value of k
#define values to check. Best to start with checking 1 through 10
knnParamSpace <- makeParamSet(makeDiscreteParam("k", values = 1:10))
#tell the computer to search every value of k we offered (1 thru 10)
gridSearch <- makeTuneControlGrid()
#cross validate for the k finding
cvForTuning <- makeResampleDesc("RepCV", folds = 10, reps =20)
#Use the tuneParams function and the task we made above to get the best k value
tunedk_RQ_2 <- suppressMessages({
tuneParams("classif.knn",
task = task_RQ_2,
resampling = cvForTuning,
par.set = knnParamSpace,
control = gridSearch)
})
tunedk_RQ_2
knn_learner_RQ_2 <- makeLearner("classif.knn", par.vals = list("k"=8))
#Running the KNN model for RQ 2
KNN_model_RQ_2 <- suppressMessages({
train(learner = knn_learner_RQ_2, task = task_RQ_2)
})
#Running cross validating
KNN_model_RQ_2_loocv <- suppressMessages({
resample(learner = knn_learner_RQ_2,
task = task_RQ_2,
resampling = loocv,
measures = list(mmce, acc))
})
saveRDS(KNN_model_RQ_2, file = "KNN_model_RQ_2.rds")
#Looking at how this model did
KNN_model_RQ_2_loocv$aggr
#Trainging the Decision Tree model for RQ #2
decision_tree_model_RQ_2 <- suppressMessages({
train(learner = decisiontree_learner_RQ_1_and_2,
task = task_RQ_2)
})
#Cross validating
decision_tree_model_RQ_2_loocv <- suppressMessages({
resample(learner = decisiontree_learner_RQ_1_and_2,
task = task_RQ_2,
resampling = loocv,
measures = list(acc))
})
#Performance of the Decision Tree
decision_tree_model_RQ_2_loocv$aggr
saveRDS(decision_tree_model_RQ_2, file = "decision_tree_model_RQ_2.rds")
#Tuning hyperparameters for SVM model
tunedSvmPars_RQ2 <- tuneParams("classif.svm", task = task_RQ_2, resampling = cvForTuning2, par.set = svm_parametercheck, control = randomsearch)
#show me what hyperparameters worked best
tunedSvmPars_RQ2
#update learner with the choosen hyperparameters
SVM_learner_tuned_RQ_2 <- makeLearner("classif.svm", par.vals = tunedSvmPars_RQ2$x)
#Run the model
SVM_model_RQ_2 <- suppressMessages({
train(learner = SVM_learner_tuned_RQ_2, task = task_RQ_2)
})
#Run Cross Validation
SVM_model_RQ_2_loocv <- suppressMessages({
resample(learner = SVM_learner_tuned_RQ_2,
task = task_RQ_2,
resampling = loocv,
measures = list(mmce, acc))
})
#performance
SVM_model_RQ_2_loocv$aggr
saveRDS(SVM_model_RQ_2, file = "SVM_model_RQ_2.rds")
#finding the best value of k
#define values to check best to check 1 through 10
knnParamSpace <- makeParamSet(makeDiscreteParam("k", values = 1:50))
#tell the computer to search every value of k we offered (1 thru 10)
gridSearch <- makeTuneControlGrid()
#cross validate for the k finding
cvForTuning <- makeResampleDesc("RepCV", folds = 10, reps =50)
#Use the tuneParams function and the task we made above to get the best k value
tunedk_RQ_1 <- tuneParams("classif.knn", task = task_RQ_3_numeric, resampling = cvForTuning, par.set = knnParamSpace, control = gridSearch)
tunedk_RQ_1
#Creating the KNN learner for RQ1
knn_learner_RQ_3 <- makeLearner("classif.knn", par.vals = list("k"=39))
#Creating the KNN learner for RQ1
knn_learner_RQ_3 <- makeLearner("classif.knn", par.vals = list("k"=35))
#Training the model
KNN_model_RQ_3 <- suppressMessages({
train(learner = knn_learner_RQ_3, task = task_RQ_3_numeric)
})
#cross validating
KNN_model_RQ_3_loocv <- suppressMessages({
resample(learner = knn_learner_RQ_3,
task = task_RQ_3_numeric,
resampling = loocv,
measures = list(acc))
})
#Preformance
KNN_model_RQ_3_loocv$aggr
saveRDS(KNN_model_RQ_3, file = "KNN_model_RQ_3.rds")
randomforest_learner_RQ_3 <- makeLearner("classif.randomForest")
getParamSet("classif.randomForest")
getParamSet("classif.randomForest")
forestParamSpace <- makeParamSet(makeIntegerParam("mtry", lower = 1, upper = 12))
randomsearch_100 <- makeTuneControlRandom(maxit = 100)
cvForTuning_randomforest <- makeResampleDesc("CV", iters = 5)
tunedForestPars <- suppressMessages({
tuneParams(learner = randomforest_learner_RQ_3,
task = task_RQ_3_factor,
resampling = cvForTuning_randomforest,
par.set = forestParamSpace,
control = randomsearch_100)
})
#Show the result
tunedForestPars
forestParamSpace <- makeParamSet(makeIntegerParam("mtry", lower = 1, upper = 12))
randomsearch_100 <- makeTuneControlRandom(maxit = 100)
cvForTuning_randomforest <- makeResampleDesc("CV", iters = 5)
tunedForestPars <- suppressMessages({
tuneParams(learner = randomforest_learner_RQ_3,
task = task_RQ_3_factor,
resampling = cvForTuning_randomforest,
par.set = forestParamSpace,
control = randomsearch_100)
})
#Show the result
tunedForestPars
#Show the result
tunedForestPars
#Update the learner
randomforest_learner_RQ_3 <- makeLearner("classif.randomForest", par.vals = list("mtry"=1))
#Train the Model
random_forest_model_RQ_3 <- suppressMessages({
train(learner = randomforest_learner_RQ_3, task = task_RQ_3_factor)
})
#Cross validating
random_forest_model_RQ_3_loocv <- suppressMessages({
resample(learner = randomforest_learner_RQ_3,
task = task_RQ_3_factor,
resampling = loocv,
measures = list(mmce, acc))
})
#Performance
random_forest_model_RQ_3_loocv$aggr
LR_learner_RQ_1 <- makeLearner("classif.logreg", predict.type = "prob")
LR_model_RQ_1 <- suppressMessages({
train(LR_learner_RQ_1, task_RQ_1)
})
LR_model_RQ_1_loocv <- suppressMessages({
resample(learner = LR_learner_RQ_1,
task = task_RQ_1,
resampling = loocv)
})
LR_model_RQ_1_loocv$aggr
NB_learner_RQ_1 <- makeLearner("classif.naiveBayes", par.vals = list("laplace" = 1))
NB_model_RQ_1 <- train(learner = NB_learner_RQ_1,
task = task_RQ_1)
NB_model_RQ_1_loocv <- suppressMessages({
resample(learner = NB_learner_RQ_1,
task = task_RQ_1,
resampling = loocv)
})
NB_model_RQ_1_loocv$aggr
library(knitr)
library(mlr)
library(tidyverse)
set.seed(1234567)
testing_data <- read.csv("testing_data.csv")
KNN_model_RQ_1 <- readRDS("KNN_model_RQ_1.rds")
decision_tree_model_RQ_1 <- readRDS("decision_tree_model_RQ_1.rds")
SVM_model_RQ_1 <- readRDS("SVM_model_RQ_1.rds")
KNN_model_RQ_2 <- readRDS("KNN_model_RQ_2.rds")
decision_tree_model_RQ_2 <- readRDS("decision_tree_model_RQ_2.rds")
SVM_model_RQ_2 <- readRDS("SVM_model_RQ_2.rds")
KNN_model_RQ_3 <- readRDS("KNN_model_RQ_3.rds")
random_forest_model_RQ_3 <- readRDS("random_forest_model_RQ_3.rds")
confusion_matrix <- data.frame(
Predicted_value = c("Model Predicted 1", "Model predicted 0"),
Actual_value_was_1 = c("True Positive", "False Negative"),
Actual_value_was_0 = c("False Positive", "True Negative"))
kable(confusion_matrix)
#Research Question #1
#extract only the two columns from the large pre-processed testing dataset we made in step 1 that are needed for the research question: training score and the ostensive binary outcome variable
testing_data_RQ_1 <- testing_data |>
select(training_score, ostensive_binary) |>
mutate(training_score = as.numeric(training_score),
ostensive_binary = as.factor(ostensive_binary))
#Research Question #2
#extract only the two columns from the large pre-processed dataset that are needed for the research question: training score and the nonostensive binary outcome variable
testing_data_RQ_2 <- testing_data |>
select(training_score, nonostensive_binary) |>
mutate(training_score = as.numeric(training_score),
nonostensive_binary = as.factor(nonostensive_binary))
#Research Question #3
#take out the two outcome variables that we don't want to use in this analysis but leave every other predictor
testing_data_RQ_3_factor <- testing_data |>
select(c(sex:miscellaneous_score, nonos_best)) |>
mutate(across(c(sex, desexed, purebred, gaze_follow, nonos_best), as.factor))
#predicting new values and getting performance metrics for all 3 models run with RQ 1
#KNN model
knn_predictions_RQ1 <- predict(KNN_model_RQ_1, newdata = testing_data_RQ_1)
performance(knn_predictions_RQ1)
calculateConfusionMatrix(knn_predictions_RQ1)
#Decision Tree
decision_tree_predictions_RQ1 <- predict(decision_tree_model_RQ_1, newdata = testing_data_RQ_1)
performance(decision_tree_predictions_RQ1)
calculateConfusionMatrix(decision_tree_predictions_RQ1)
#SVM
SVM_predictions_RQ_1 <- predict(SVM_model_RQ_1, newdata = testing_data_RQ_1)
performance(SVM_predictions_RQ_1)
calculateConfusionMatrix(SVM_predictions_RQ_1)
#predicting new values and getting performance metrics for all 3 models run with RQ 2
#KNN model
knn_predictions_RQ2 <- predict(KNN_model_RQ_2, newdata = testing_data_RQ_2)
performance(knn_predictions_RQ2)
calculateConfusionMatrix(knn_predictions_RQ2)
#Decision Tree
decision_tree_predictions_RQ2 <- predict(decision_tree_model_RQ_2, newdata = testing_data_RQ_2)
performance(decision_tree_predictions_RQ2)
calculateConfusionMatrix(decision_tree_predictions_RQ2)
#SVM
SVM_predictions_RQ2 <- predict(SVM_model_RQ_2, newdata = testing_data_RQ_2)
performance(SVM_predictions_RQ2)
calculateConfusionMatrix(SVM_predictions_RQ2)
#predicting new values and getting performance metrics for all 3 models run with RQ 2
#KNN model
knn_predictions_RQ3 <- predict(KNN_model_RQ_3, newdata = testing_data_RQ_3_factor)
performance(knn_predictions_RQ3)
#Dig deeper into predictions with a confusion matrix
calculateConfusionMatrix(knn_predictions_RQ3)
#Random Forest
randomforest_predictions_RQ3 <- predict(random_forest_model_RQ_3, newdata = testing_data_RQ_3_factor)
#Random Forest
randomforest_predictions_RQ3 <- predict(random_forest_model_RQ_3, newdata = testing_data_RQ_3_factor)
View(testing_data_RQ_3_factor)
View(random_forest_model_RQ_3)
#Update the learner
randomforest_learner_RQ_3 <- makeLearner("classif.randomForest", par.vals = list("mtry"=1))
#Train the Model
random_forest_model_RQ_3 <- suppressMessages({
train(learner = randomforest_learner_RQ_3, task = task_RQ_3_factor)
})
#Cross validating
random_forest_model_RQ_3_loocv <- suppressMessages({
resample(learner = randomforest_learner_RQ_3,
task = task_RQ_3_factor,
resampling = loocv,
measures = list(mmce, acc))
})
saveRDS(random_forest_model_RQ_3, file = "random_forest_model_RQ_3.rds")
#Performance
random_forest_model_RQ_3_loocv$aggr
library(knitr)
library(mlr)
library(tidyverse)
set.seed(1234567)
testing_data <- read.csv("testing_data.csv")
KNN_model_RQ_1 <- readRDS("KNN_model_RQ_1.rds")
decision_tree_model_RQ_1 <- readRDS("decision_tree_model_RQ_1.rds")
SVM_model_RQ_1 <- readRDS("SVM_model_RQ_1.rds")
KNN_model_RQ_2 <- readRDS("KNN_model_RQ_2.rds")
decision_tree_model_RQ_2 <- readRDS("decision_tree_model_RQ_2.rds")
SVM_model_RQ_2 <- readRDS("SVM_model_RQ_2.rds")
KNN_model_RQ_3 <- readRDS("KNN_model_RQ_3.rds")
random_forest_model_RQ_3 <- readRDS("random_forest_model_RQ_3.rds")
#Random Forest
randomforest_predictions_RQ3 <- predict(random_forest_model_RQ_3, newdata = testing_data_RQ_3_factor)
#Measure how well the model did at predictions
performance(randomforest_predictions_RQ3)
#Dig deeper into predictions with a confusion matrix
calculateConfusionMatrix(randomforest_predictions_RQ3)
assumptiontable <- data.frame(
Model = c("Logistic Regression", "KNN", "Naive Bayes", "Decision Trees", "Support Vector Machines"),
Independence = c("Yes", "No", "Yes", "No", "No"),
Normality = c("No", "No", "Yes", "No", "No"),
No Outliers = c("Yes", "No", "No", "No", "No"),
assumptiontable <- data.frame(
Model = c("Logistic Regression", "KNN", "Naive Bayes", "Decision Trees", "Support Vector Machines"),
Independence = c("Yes", "No", "Yes", "No", "No"),
Normality = c("No", "No", "Yes", "No", "No"),
No_Outliers = c("Yes", "No", "No", "No", "No"),
Linearity = c("Yes", "No", "No", "No", "No"),
High_Dimensionality = c("No", "Yes", "No", "No", "No"),
Feature_Scaling = c("Yes","Yes","No","No","Yes"))
kable(assumptiontable)
